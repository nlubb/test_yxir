{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e93ac4-7ccc-41cf-88ed-204adadec9e5",
   "metadata": {},
   "source": [
    "# Etape 3 : Construction d'un modèle amélioré exploitant les sections\n",
    "\n",
    "Pluôt que de prendre brutalement les embeddings des textes comme dans l'étape 2, nous faisons le travail suivant sur le text:\n",
    "- pour chaque section, on chunk des phrases de tailles < input du LLM\n",
    "- on réalise les embeddings de section par mean pooling des embeddings des phrases de section\n",
    "- on réalise l'embedding du brevet par mean pooling des embeddings des sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c6d83b-ddc7-4b7d-b09d-8f82dd542873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "from sentence_transformers import losses\n",
    "from sentence_transformers.readers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf461ca7-c1d6-4ba6-83da-5a7a14fa5ca4",
   "metadata": {},
   "source": [
    "### Creation des embeddings par groupe de phrases sur le brevet entier\n",
    "On divise chaque section du brevet en phrases de nb_tokens < 512 pour réaliser les embeddings des phrases. Puis on moyenne les embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48127abe-cc03-4698-9d3d-09708179a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/dataset_patent_sections.json', 'r') as outfile:\n",
    "    dataset_patent_section = json.load(outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a501a2ea-c91b-49fd-9ad7-08c9294b5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_sentences_from_tokenizer(text, tokenizer, dimensions = 512):\n",
    "    '''\n",
    "    Recuperation de la liste de phrases qui ont une taille inferieure a la dimension du LLM\n",
    "    section -- str, le texte dont on veut recuperer la liste de phrases\n",
    "    '''\n",
    "    list_tokens = tokenizer.tokenize(text)\n",
    "    list_sentences = []\n",
    "    for i in range(int(len(list_tokens)/dimensions)+1):\n",
    "        sentence = ' '.join(list_tokens[dimensions*i:dimensions*(i+1)])\n",
    "        list_sentences.append(sentence)\n",
    "    return list_sentences\n",
    "\n",
    "def get_patent_text_sentences_from_tokenizer(patent, tokenizer, dimensions = 512):\n",
    "    '''\n",
    "    Recuperation de la list de phrases constituant le brevet. Ici le brevt est une liste de sections\n",
    "    patent -- list, representation du dictionnaire en sections\n",
    "    '''\n",
    "    list_sentences = []\n",
    "    for i in range(len(patent)):\n",
    "        text = patent[i]['content']\n",
    "        list_sentences_text = get_text_sentences_from_tokenizer(text, \n",
    "                                                                tokenizer, \n",
    "                                                                dimensions = dimensions)\n",
    "        list_sentences += list_sentences_text\n",
    "    return list_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "28c51d32-cc19-48ba-852c-919f50860a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56b2105bf37430ea425378f3cfeca00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bba29cede34279a4d81f505d40d6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2f245dd73646fea99ffc48ace18cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436bb20d9c374f0b821e629f1a010ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273df5a991954be480ab2849edd2c5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1b63b57e7b4b1c8c822a5603966e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/67.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c2657df4674fd5a08b8d07e641c48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6571a7243e4372b64e15426136f262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e933d3f7439a4d239a33992e65ac3ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee077c504dba4ec7991adde69c2d6663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul des embeddings initiaux:   0%|                                                                                                                                             | 0/499 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Calcul des embeddings initiaux: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [34:36<00:00,  4.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# Calcul des embeddings de tous les brevets comme moyenne des embeddings de phrases\n",
    "\n",
    "# model_name = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "model_name = 'intfloat/e5-small-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dimensions = 512\n",
    "model = SentenceTransformer(model_name, truncate_dim=dimensions, revision=None)\n",
    "\n",
    "dataset_patent_section_embeddings = {}\n",
    "for i in tqdm(range(len(dataset_patent_section)), desc =\"Calcul des embeddings initiaux\"):\n",
    "    dataset_patent_section_embeddings[str(i)] = {}\n",
    "    for key in ['pos', 'negative']:\n",
    "        list_sentences = get_patent_text_sentences_from_tokenizer(dataset_patent_section[str(i)][key], \n",
    "                                                                  tokenizer, \n",
    "                                                                  dimensions = int(dimensions/2))\n",
    "        embeddings = model.encode(list_sentences)\n",
    "        patent_embedding = np.mean(embeddings, axis=0)\n",
    "        dataset_patent_section_embeddings[str(i)][key] = patent_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "62c2b161-1c00-49cf-befe-b85163c45971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul des embeddings des query: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:10<00:00, 46.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Ajout de l'embedding des query\n",
    "for i in tqdm(range(len(dataset_patent_section)), desc =\"Calcul des embeddings des query\"):\n",
    "    list_sentences = [dataset_patent_section[str(i)]['query']]\n",
    "    embeddings = model.encode(list_sentences)\n",
    "    patent_embedding = np.mean(embeddings, axis=0)\n",
    "    dataset_patent_section_embeddings[str(i)]['query'] = patent_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cbcc6c2e-9057-4ada-921d-506e5b5ff195",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/patent_sections_embeddings_e5-small-v2.pickle', 'wb') as fh:\n",
    "    pickle.dump(dataset_patent_section_embeddings, fh)\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d6853e-207b-4642-93c8-e1ea6bfb2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/patent_sections_embeddings_e5-small-v2.pickle', 'rb') as fh:\n",
    "    dataset_patent_section_embeddings = pickle.load(fh)\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "873ab91d-f812-4f8c-951e-f81672c79a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings de documents compatibles avec la query: 450, 90.18 %\n"
     ]
    }
   ],
   "source": [
    "# Performances -- classification\n",
    "nb_good_embeddings = 0\n",
    "for i in range(len(dataset_patent_section_embeddings)):\n",
    "    embeddings = [\n",
    "        dataset_patent_section_embeddings[str(i)]['query'],\n",
    "        dataset_patent_section_embeddings[str(i)]['pos'],\n",
    "        dataset_patent_section_embeddings[str(i)]['negative']\n",
    "    ]\n",
    "    similarities = cos_sim(embeddings[0], embeddings[1:])\n",
    "    sim_pos, sim_neg = similarities.flatten()\n",
    "    if sim_pos > sim_neg :\n",
    "        nb_good_embeddings+=1\n",
    "perc_good_embeddings = round(100*nb_good_embeddings/len(dataset_patent_section_embeddings),2)\n",
    "print('Embeddings de documents compatibles avec la query: {}, {} %'.format(nb_good_embeddings,\n",
    "                                                                           perc_good_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d80f12e-b742-48b9-bd38-e7030e67b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul du top_K_accuracy score: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:43<00:00, 11.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_K_pos : 0.9338677354709419 (a maximiser)\n",
      "acc_K_neg : 0.4168336673346693 (a minimiser)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Performances -- top_K_accuracy\n",
    "list_all_embeddings = []\n",
    "for i in range(len(dataset_patent_section_embeddings)):\n",
    "    emb_query = dataset_patent_section_embeddings[str(i)]['query']\n",
    "    emb_pos = dataset_patent_section_embeddings[str(i)]['pos']\n",
    "    emb_neg = dataset_patent_section_embeddings[str(i)]['negative']\n",
    "    list_all_embeddings.append([emb_query, emb_pos, emb_neg])\n",
    "\n",
    "def compute_top_K_accuracy_score(list_embeddings, K=5):\n",
    "    '''\n",
    "    Fonction pour calculer le top_K_accuracy score a partir d'une liste d'embeddings de type :\n",
    "    [[emb_query, emb_pos, emb_neg]...]\n",
    "    list_embeddings -- list, list des embeddings des query, positive, negative\n",
    "    K -- int, le top K accuracy\n",
    "    '''\n",
    "    list_embeddings_query = [list_embeddings[i][0] for i in range(len(list_all_embeddings))]\n",
    "    list_embeddings_pos = [list_embeddings[i][1] for i in range(len(list_all_embeddings))]\n",
    "    list_embeddings_neg = [list_embeddings[i][2] for i in range(len(list_all_embeddings))]\n",
    "    \n",
    "    nb_pos = 0\n",
    "    nb_neg = 0\n",
    "    for idx in tqdm(range(len(list_embeddings_query)), desc =\"Calcul du top_K_accuracy score\"):\n",
    "        query = list_embeddings_query[idx]\n",
    "        \n",
    "        similarities_pos = cos_sim(query, list_embeddings_pos).flatten().tolist()\n",
    "        similarities_pos = [('pos_{}'.format(i), similarities_pos[i]) for i in range(len(similarities_pos))]\n",
    "        \n",
    "        similarities_neg = cos_sim(query, list_embeddings_neg).flatten().tolist()\n",
    "        similarities_neg = [('neg_{}'.format(i), similarities_neg[i]) for i in range(len(similarities_neg))]\n",
    "        \n",
    "        similarities = similarities_pos+similarities_neg\n",
    "        similarities = sorted(similarities, key = lambda x: -x[1])\n",
    "        top_K_ids = [similarities[i][0] for i in range(K)]\n",
    "        \n",
    "        if 'pos_{}'.format(idx) in top_K_ids:\n",
    "            nb_pos+=1\n",
    "        if 'neg_{}'.format(idx) in top_K_ids:\n",
    "            nb_neg+=1\n",
    "    acc_K_pos = nb_pos/len(list_embeddings)\n",
    "    acc_K_neg = nb_neg/len(list_embeddings)\n",
    "    return acc_K_pos, acc_K_neg\n",
    "\n",
    "acc_K_pos, acc_K_neg = compute_top_K_accuracy_score(list_all_embeddings, K=5)\n",
    "print('acc_K_pos : {} (a maximiser)'.format(acc_K_pos))\n",
    "print('acc_K_neg : {} (a minimiser)'.format(acc_K_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc0ea6-389a-435a-8888-3afea784046b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yxir",
   "language": "python",
   "name": "yxir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
