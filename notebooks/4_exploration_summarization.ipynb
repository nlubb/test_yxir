{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64b9759-5e2c-476b-b8ee-4a2f3e939ed3",
   "metadata": {},
   "source": [
    "# Etape 4 : Construction d'un modèle en passant par des résumés de section\n",
    "\n",
    "La démarche ici est la suivante : \n",
    "- pour chaque section de brevet on réalise un résumé à base des 5 phrases les plus importantes de la section\n",
    "- on réalise l'embedding de la section par mean pooling des embeddings des phrases résumant la section\n",
    "- on réalise l'embedding du brevet par mean pooling des embeddings des résumés de section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a7c882b-21e6-4566-b313-802c0bd385d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "from sentence_transformers import losses\n",
    "from sentence_transformers.readers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from LexRank import degree_centrality_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f8361b-7c19-46e9-8bcb-744fd85d57fe",
   "metadata": {},
   "source": [
    "### Création des embeddings après avoir appliqué des résumés de chacune des sections\n",
    "Le but ici est d'utiliser un LLM pour créer des résumés de chaque section, puis de créer les embeddings des résumés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239c0ace-c426-4748-a177-09f5b49561ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/dataset_patent_sections.json', 'r') as outfile:\n",
    "    dataset_patent_section = json.load(outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60cd5e8c-708b-44e3-aef9-1ea3f038b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'intfloat/e5-small-v2'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6a878346-6b92-491b-ba2e-54694a358267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construction du dataset de resumes: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [32:49<00:00,  3.95s/it]\n"
     ]
    }
   ],
   "source": [
    "# Construction du dataset de brevets resumes par section\n",
    "\n",
    "def summarize_vanilla(text, model, max_sentences=5):\n",
    "    '''\n",
    "    Fonction pour fournir le resume d'un texte sur la base des ses phrases les plus importantes\n",
    "    text -- str, le texte a resumer\n",
    "    model -- SentenceTransformer, model de calcul des embeddings\n",
    "    max_sentences -- int, le nombre de phrases max dans le resume\n",
    "    '''\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    embeddings = model.encode(sentences)\n",
    "    similarity_scores = cos_sim(embeddings, embeddings).numpy()\n",
    "    centrality_scores = degree_centrality_scores(similarity_scores, threshold=None)\n",
    "    most_central_sentence_indices = np.argsort(-centrality_scores)\n",
    "    nb_sentences_summary = min(5, len(sentences))\n",
    "    list_sentences_summary = []\n",
    "    list_embeddings_summary = []\n",
    "    for idx in most_central_sentence_indices[0:nb_sentences_summary]:\n",
    "        list_sentences_summary.append(sentences[idx].strip())\n",
    "        list_embeddings_summary.append(embeddings[idx])\n",
    "    summary_embedding = np.mean(list_embeddings_summary, axis=0)\n",
    "    summary = ' '.join(list_sentences_summary)\n",
    "    return summary, summary_embedding\n",
    "\n",
    "dataset_patent_section_summary = {}\n",
    "for i in tqdm(range(len(dataset_patent_section)), desc =\"Construction du dataset de resumes\"):\n",
    "    dict_patent = {}\n",
    "    dict_patent['query'] = dataset_patent_section[str(i)]['query']\n",
    "    for key in ['pos', 'negative']:\n",
    "        list_sections = dataset_patent_section[str(i)][key]\n",
    "        list_sections_summary = []\n",
    "        for j in range(len(list_sections)):\n",
    "            text = list_sections[j]['content']\n",
    "            summary, summary_embedding = summarize_vanilla(text, model, max_sentences=5)\n",
    "            list_sections_summary.append({'section': list_sections[j]['section'],\n",
    "                                          'content': summary,\n",
    "                                          'embedding': summary_embedding})\n",
    "        dict_patent[key] = list_sections_summary\n",
    "    dataset_patent_section_summary[str(i)] = dict_patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "64f33638-d32a-41f0-ba27-6a1889773294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construction des embeddings de brevets: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:00<00:00, 16355.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Construction des embeddings de brevet a partir des embeddings des resumes de sections\n",
    "dataset_patent_section_embeddings = {}\n",
    "\n",
    "for i in tqdm(range(len(dataset_patent_section_summary)), desc =\"Construction des embeddings de brevets\"):\n",
    "    dataset_patent_section_embeddings[str(i)] = {}\n",
    "    for key in ['pos', 'negative']:\n",
    "        list_sections_dict = dataset_patent_section_summary[str(i)][key]\n",
    "        list_embeddings = [list_sections_dict[j]['embedding'] for j in range(len(list_sections_dict))]\n",
    "        patent_embedding = np.mean([emb for emb in list_embeddings if (not isinstance(emb, float))], axis=0)\n",
    "        dataset_patent_section_embeddings[str(i)][key] = patent_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d0dc1dac-2bf9-4b23-8533-38830ac0f812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul des embeddings des query: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:10<00:00, 47.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Ajout des embeddings de query\n",
    "for i in tqdm(range(len(dataset_patent_section_embeddings)), desc =\"Calcul des embeddings des query\"):\n",
    "    list_sentences = [dataset_patent_section[str(i)]['query']]\n",
    "    embeddings = model.encode(list_sentences)\n",
    "    query_embedding = np.mean(embeddings, axis=0)\n",
    "    dataset_patent_section_embeddings[str(i)]['query'] = query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a951a322-b022-4902-b0c3-4b5555fe9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/patent_sections_embeddings_summary_e5-small-v2.pickle', 'wb') as fh:\n",
    "    pickle.dump(dataset_patent_section_embeddings, fh)\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1cefe9e-c8cc-46c1-8f59-3efbff54fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/patent_sections_embeddings_summary_e5-small-v2.pickle', 'rb') as fh:\n",
    "    dataset_patent_section_embeddings = pickle.load(fh)\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1553eff8-131f-401d-8af3-8ca607137693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings de documents compatibles avec la query: 408, 81.76 %\n"
     ]
    }
   ],
   "source": [
    "# Performances -- classification\n",
    "nb_good_embeddings = 0\n",
    "for i in range(len(dataset_patent_section_embeddings)):\n",
    "    embeddings = [\n",
    "        dataset_patent_section_embeddings[str(i)]['query'],\n",
    "        dataset_patent_section_embeddings[str(i)]['pos'],\n",
    "        dataset_patent_section_embeddings[str(i)]['negative']\n",
    "    ]\n",
    "    similarities = cos_sim(embeddings[0], embeddings[1:])\n",
    "    sim_pos, sim_neg = similarities.flatten()\n",
    "    if sim_pos > sim_neg :\n",
    "        nb_good_embeddings+=1\n",
    "perc_good_embeddings = round(100*nb_good_embeddings/len(dataset_patent_section_embeddings),2)\n",
    "print('Embeddings de documents compatibles avec la query: {}, {} %'.format(nb_good_embeddings,\n",
    "                                                                           perc_good_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5533c3ab-b9a0-4709-8da0-82a95de823dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul du top_K_accuracy score: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:44<00:00, 11.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_K_pos : 0.8837675350701403 (a maximiser)\n",
      "acc_K_neg : 0.5811623246492986 (a minimiser)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Performances -- top_K_accuracy\n",
    "list_all_embeddings = []\n",
    "for i in range(len(dataset_patent_section_embeddings)):\n",
    "    emb_query = dataset_patent_section_embeddings[str(i)]['query']\n",
    "    emb_pos = dataset_patent_section_embeddings[str(i)]['pos']\n",
    "    emb_neg = dataset_patent_section_embeddings[str(i)]['negative']\n",
    "    list_all_embeddings.append([emb_query, emb_pos, emb_neg])\n",
    "\n",
    "def compute_top_K_accuracy_score(list_embeddings, K=5):\n",
    "    '''\n",
    "    Fonction pour calculer le top_K_accuracy score a partir d'une liste d'embeddings de type :\n",
    "    [[emb_query, emb_pos, emb_neg]...]\n",
    "    list_embeddings -- list, list des embeddings des query, positive, negative\n",
    "    K -- int, le top K accuracy\n",
    "    '''\n",
    "    list_embeddings_query = [list_embeddings[i][0] for i in range(len(list_all_embeddings))]\n",
    "    list_embeddings_pos = [list_embeddings[i][1] for i in range(len(list_all_embeddings))]\n",
    "    list_embeddings_neg = [list_embeddings[i][2] for i in range(len(list_all_embeddings))]\n",
    "    \n",
    "    nb_pos = 0\n",
    "    nb_neg = 0\n",
    "    for idx in tqdm(range(len(list_embeddings_query)), desc =\"Calcul du top_K_accuracy score\"):\n",
    "        query = list_embeddings_query[idx]\n",
    "        \n",
    "        similarities_pos = cos_sim(query, list_embeddings_pos).flatten().tolist()\n",
    "        similarities_pos = [('pos_{}'.format(i), similarities_pos[i]) for i in range(len(similarities_pos))]\n",
    "        \n",
    "        similarities_neg = cos_sim(query, list_embeddings_neg).flatten().tolist()\n",
    "        similarities_neg = [('neg_{}'.format(i), similarities_neg[i]) for i in range(len(similarities_neg))]\n",
    "        \n",
    "        similarities = similarities_pos+similarities_neg\n",
    "        similarities = sorted(similarities, key = lambda x: -x[1])\n",
    "        top_K_ids = [similarities[i][0] for i in range(K)]\n",
    "        \n",
    "        if 'pos_{}'.format(idx) in top_K_ids:\n",
    "            nb_pos+=1\n",
    "        if 'neg_{}'.format(idx) in top_K_ids:\n",
    "            nb_neg+=1\n",
    "    acc_K_pos = nb_pos/len(list_embeddings)\n",
    "    acc_K_neg = nb_neg/len(list_embeddings)\n",
    "    return acc_K_pos, acc_K_neg\n",
    "\n",
    "acc_K_pos, acc_K_neg = compute_top_K_accuracy_score(list_all_embeddings, K=5)\n",
    "print('acc_K_pos : {} (a maximiser)'.format(acc_K_pos))\n",
    "print('acc_K_neg : {} (a minimiser)'.format(acc_K_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb19dcc-a481-4531-94c6-04d99b9c9ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yxir",
   "language": "python",
   "name": "yxir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
