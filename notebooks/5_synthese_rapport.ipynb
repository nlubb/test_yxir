{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9531ad-977d-42e8-918a-8c15db6eac02",
   "metadata": {},
   "source": [
    "# Synthèse et conclusions\n",
    "\n",
    "#### Jeu de données\n",
    "\n",
    "Durant ce test, nous avons exploré un jeu de données composés de contenus de brevet et de leurs résumés. Le jeu de données est construit sous forme de triplet (query, positive_patent, negative_patent) où le positive_patent est sensé être le brevet le plus proche a retrouver a partir de la query, et le negative_patent un brevet proche en terme de contenu, mais qui ne correspond pas a l'objet de la query.\n",
    "\n",
    "#### Problème à résoudre\n",
    "\n",
    "Le problème à résoudre est de trouver une manière de réaliser les embeddings des brevets de telle sorte que les exemples positifs soit plus similaires à la query que les exemples négatifs. De plus, dans une tache de retrieval, pour la query donnee, il faut que l'exemple positif arrive dans le top K, contrairement a l'exemple négatif.\n",
    "\n",
    "#### 1- Exploration des données\n",
    "\n",
    "L'exploration des données nous a permis de faire notamment les observations suivantes: \n",
    "- le jeu de données est composé de contenus de brevet et d'abstracts de brevets\n",
    "- la longueur des documents est importante, il faut donc réaliser du chunk avant d'ingérer les données dans un LLM\n",
    "\n",
    "#### 2- Méthodes testées \n",
    "\n",
    "Nous avons testé trois méthodes pour réaliser l'embedding des brevets, toutes utilisant le modèle intfloat/e5-small-v2 :\n",
    "- méthode 1: on fait l'embedding directement sur tout le contenu du document (méthode zero-shot). On teste aussi la variante en finetunant le modèle avec la contrastive loss.\n",
    "- méthode 2 : on fait l'embedding des brevets a partir des embeddings de sections en realisant un max pooling. Chaque embedding de section etant lui-même obtenu par max pooling des embeddings de chunks composant la section\n",
    "- méthode 3 : on fait l'embedding des brevets a partir des embeddings de résumés de sections en réalisant un max pooling. Les résumés de section étant constitué des 5 phrases les plus importantes constituant la section\n",
    "\n",
    "#### 3- résultats obtenus\n",
    "\n",
    "- méthode 1 Zero-Shot: \n",
    "    - Accuracy : 74.15 %\n",
    "    - top_5_accuracy positive : 93.4 %\n",
    "    - top_5_accuracy negative : 71.3 %\n",
    "- méthode 1 Finetuning (probleme d'overfitting) : \n",
    "    - Accuracy : 100 %\n",
    "    - top_5_accuracy positive : 19.4 %\n",
    "    - top_5_accuracy negative : 0 %\n",
    "- méthode 2: \n",
    "    - Accuracy : 90.18 %\n",
    "    - top_5_accuracy positive : 93.4 %\n",
    "    - top_5_accuracy negative : 41.7 %\n",
    "- méthode 3: \n",
    "    - Accuracy : 81.76 %\n",
    "    - top_5_accuracy positive : 88.37 %\n",
    "    - top_5_accuracy negative : 58.1 %\n",
    "\n",
    "#### 4- Interprétation des résultats\n",
    "\n",
    "La méthode 2 donne les meilleures résultats. Elle a nécessité un travail plus approfondi pour comprendre les brevets en exploitant les sections.\n",
    "\n",
    "Les résultats de la méthode 3 ne sont pas si satisfaisants car la manière de générer les résumés n'est efficace. Il faudrait plutôt générer des vrais résumés à partir de LLM entrainés sur une tâche de summarization, plutôt que de prendre directement les phrases du document.\n",
    "\n",
    "#### 5- Idées a explorer pour la suite\n",
    "\n",
    "Les idées qui viennent tout de suite pour une future exploration sont les suivantes : \n",
    "- Tester des LLM plus performants\n",
    "- Résumer chaque section directement par un LLM\n",
    "- Faire le finetuning des modèles en exploitant la contrastive loss sur les chunks construits avec la méthode 2 plutôt qu'en brute force directe sur tout le document\n",
    "- Avoir un plus gros dataset, avec plus d'exemples négatifs pour chaque query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda73caa-4c17-4685-b2fc-61a48c6b783f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yxir",
   "language": "python",
   "name": "yxir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
